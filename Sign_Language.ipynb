{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4Iu0d1Gvp4isiU1MpCrf3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MowsamR/Sign-Language/blob/main/Sign_Language.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3hplSvKcxMT",
        "outputId": "6d50de0e-aaf4-4b09-de7d-4ab5b5e9f87a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.9/dist-packages (4.7.0.72)\n",
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.9.2.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.6/33.6 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from opencv-python) (1.22.4)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.9/dist-packages (from mediapipe) (22.2.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from mediapipe) (23.3.3)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.9/dist-packages (from mediapipe) (4.7.0.72)\n",
            "Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.9/dist-packages (from mediapipe) (3.20.3)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (1.0.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (23.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (5.12.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (8.4.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Installing collected packages: mediapipe\n",
            "Successfully installed mediapipe-0.9.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python mediapipe matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "import mediapipe as mp\n",
        "import tensorflow as tf\n"
      ],
      "metadata": {
        "id": "OVV5_LfCd06r"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mp_holistic = mp.solutions.holistic # Holistic model\n",
        "mp_drawing = mp.solutions.drawing_utils # Drawing utilitie"
      ],
      "metadata": {
        "id": "z3RtEi_ld260"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mediapipe_detection(image, model):\n",
        "\timage = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
        "\timage.flags.writeable = False\n",
        "\tresults = model.process(image)\n",
        "\timage.flags.writeable = True\n",
        "\timage = cv.cvtColor(image, cv.COLOR_RGB2BGR)\n",
        "\treturn image, results"
      ],
      "metadata": {
        "id": "JDb5wQyed4OQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_landmarks(image, results):\n",
        "\tmp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1))\n",
        "\tmp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2))\n",
        "\tmp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2))\n",
        "\tmp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2))"
      ],
      "metadata": {
        "id": "kKuQAOcTd5IH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_keypoints(results):\n",
        "\tpose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
        "\tface = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
        "\tleft_hand = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
        "\tleft_hand = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
        "\treturn np.concatenate([pose, face, left_hand, left_hand])\n"
      ],
      "metadata": {
        "id": "1-mM0p4Zd7P7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
        "\t\tsamples_path = os.path.join('Sign', 'Clips')\n",
        "\t\tshould_break = False\n",
        "\t\tfor sample in os.listdir(samples_path):\n",
        "\t\t\tif should_break:\n",
        "\t\t\t\tbreak\n",
        "\n",
        "\t\t\tprint(sample)\n",
        "\t\t\tclips_path = os.path.join(samples_path, sample) \n",
        "      #clips_path = os.path.join(samples_path, sample): This line creates\n",
        "      #a string containing the path to the current file or directory by joining the 'Clips' directory\n",
        "      #path with the name of the current file or directory.\n",
        "\t\t\tif not os.path.isdir(clips_path):\n",
        "\t\t\t\tcontinue\n",
        "\n",
        "\t\t\tfor clip_number, clip in enumerate(os.listdir(clips_path)):\n",
        "\t\t\t\tif should_break:\n",
        "\t\t\t\t\tbreak\n",
        "\n",
        "\t\t\t\tfile = os.path.join(clips_path, clip)\n",
        "\t\t\t\tif not (os.path.isfile(file) and file.endswith('.mp4')):\n",
        "\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\tcap = cv.VideoCapture(file)\n",
        "\n",
        "\t\t\t\tif not cap.isOpened(): \n",
        "\t\t\t\t\tprint(\"Cannot open Video feed\")\n",
        "\t\t\t\t\texit()\n",
        "\t\t\t\tframe_number = 0\n",
        "\t\t\t\twhile True:\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\t# Capture frame-by-frame\n",
        "\t\t\t\t\tret, frame = cap.read()\n",
        "\t\t\t\t\t    \n",
        "\t\t\t\t\t# if frame is read correctly ret is True\n",
        "\t\t\t\t\tif not ret:\n",
        "\t\t\t\t\t\tprint(\"Can't receive frame (stream end?). Exiting ...\")\n",
        "\t\t\t\t\t\tbreak\n",
        "\n",
        "\t\t\t\t\t# Make detections\n",
        "\t\t\t\t\timage, results = mediapipe_detection(frame, holistic)\n",
        "\n",
        "\t\t\t\t\tkey_points = extract_keypoints(results)\n",
        "\t\t\t\t\tprint(key_points)\n",
        "\t\t\t\t\tnpy_path = os.path.join('Dataset', str(sample), str(clip_number), str(frame_number))\n",
        "\t\t\t\t\tnp.save(npy_path, key_points)\n",
        "\n",
        "\t\t\t\t\tdraw_landmarks(image, results)\n",
        "\n",
        "\t\t\t\t\t# Display the resulting frame \n",
        "\t\t\t\t\timage = cv.flip(image, 1)\n",
        "\t\t\t\t\tcv2_imshow(image)\n",
        "\n",
        "\n",
        "\t\t\t\t\tif cv.waitKey(1) == ord('q'):\n",
        "\t\t\t\t\t\tshould_break = True\n",
        "\t\t\t\t\t\tbreak\n",
        "\n",
        "\t\t\t\t\tframe_number += 1\n",
        "\n",
        "\t\t\t\t# When everything done, release the capture\n",
        "\t\t\t\tcap.release() \n",
        "\t\t\t\tcv.destroyAllWindows()"
      ],
      "metadata": {
        "id": "c5ebwRP2d9lg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "!git clone https://github.com/MowsamR/Sign-Language.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qy16zjneeAYr",
        "outputId": "3c017b57-fc04-49b2-d287-8e667de458bb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Sign-Language'...\n",
            "remote: Enumerating objects: 4671, done.\u001b[K\n",
            "remote: Counting objects: 100% (4671/4671), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4643/4643), done.\u001b[K\n",
            "remote: Total 4671 (delta 28), reused 4667 (delta 27), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (4671/4671), 29.29 MiB | 14.51 MiB/s, done.\n",
            "Resolving deltas: 100% (28/28), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path for exported data, numpy arrays\n",
        "DATA_PATH = os.path.join('/content', 'Sign-Language', 'Extracted_Keypoints') \n",
        "\n",
        "# Actions that we try to detect\n",
        "actions = np.array(['Good', 'Hello', 'Thank_you'])\n",
        "\n",
        "# This is the number of videos per action \n",
        "no_sequences = 50\n",
        "\n",
        "# Each video has 30 frames\n",
        "sequence_length = 30\n",
        "\n",
        "label_map = {label:num for num, label in enumerate(actions)}\n",
        "\n"
      ],
      "metadata": {
        "id": "HqLDZ1vugJTu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sequences contains \n",
        "sequences, labels = [], []\n",
        "#Loop through each action\n",
        "for action in actions:\n",
        "  #Loop through each video (no_sequence = video)\n",
        "    for sequence in range(1,no_sequences+1):\n",
        "        window = []\n",
        "        #Loop through each frame in each video (sequence_length = frame)\n",
        "        for frame_num in range(sequence_length):\n",
        "            #Load the data in \"{}.npy\".format(frame_num) into res variable.\n",
        "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
        "            #all frames collected will be stored in window list sequentially until (0-29)\n",
        "            window.append(res)\n",
        "\n",
        "        #once the window list is full (contains 0-29 .npy files), the loop will start again from no_sequence videos.\n",
        "        #all the 0-29 .npy files stored in windows will then be stored in sequences lists in that order until (1-50)\n",
        "        sequences.append(window)\n",
        "\n",
        "        #label_map[action] gets the index for the current action.\n",
        "        #this index will be appended to labels list\n",
        "        labels.append(label_map[action])"
      ],
      "metadata": {
        "id": "j66rX4mfkXc1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(sequences).shape\n",
        "#150 videos (50 videos each x 3 actions = 150)\n",
        "#30 frames per video\n",
        "#1662 landmarks per frame"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qcabmcQ1von",
        "outputId": "699f50d2-3f01-4139-8e61-e8fa9d23ffd9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(150, 30, 1662)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the landmarks stored in seqeunces list will be the x-axis\n",
        "X = np.array(sequences)\n",
        "X.shape"
      ],
      "metadata": {
        "id": "fmG4KEc_3L5q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dee48bba-e3e7-4fe2-d5bf-67d5d3a73a19"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(150, 30, 1662)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The actions in labels list will be the y-axis\n",
        "# First, need to convert the actions into one-hot encoded vector.\n",
        "#A one-hot encoded vector is a binary representation of a categorical variable, where each category is represented as a vector of 0's and 1's.\n",
        "y = to_categorical(labels).astype(int)\n",
        "\n",
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mt-G7JaTM3Nf",
        "outputId": "dc98abfa-771f-45bc-ac53-a607d8c96eb2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(150, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Spliting dataset into training and testing split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojOKilp0M69X",
        "outputId": "8c3d2426-a8c7-4171-99af-9bb3f9beeec2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (105, 30, 1662)\n",
            "y_train shape: (105, 3)\n",
            "X_test shape: (45, 30, 1662)\n",
            "y_test shape: (45, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.callbacks import TensorBoard"
      ],
      "metadata": {
        "id": "fWzX7f3jNyP8"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for monitoring LSTM training for accuracy\n",
        "log_dir = os.path.join('/content/Logs')\n",
        "tb_callback = TensorBoard(log_dir=log_dir)"
      ],
      "metadata": {
        "id": "Nn49O0FaOfNE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n"
      ],
      "metadata": {
        "id": "P4trxFeOxxQf"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,1662)))\n",
        "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
        "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(actions.shape[0], activation='softmax'))\n"
      ],
      "metadata": {
        "id": "Nm9A0Z3iel4a"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n"
      ],
      "metadata": {
        "id": "XYKdIrDOg6Wu"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, epochs=1000, callbacks=[tb_callback])\n"
      ],
      "metadata": {
        "id": "Rsn8p2FPg_f7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M900yFJK5SEQ",
        "outputId": "59063d6d-2052-4c75-d0d7-2b0dd3a16c6b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 30, 64)            442112    \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 30, 128)           98816     \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 64)                49408     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 3)                 99        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 596,675\n",
            "Trainable params: 596,675\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing our video\n",
        "res = model.predict(X_test)\n",
        "res.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzLnnEULRWiH",
        "outputId": "6751ba2a-e06a-4be4-bb18-d539617c7b1d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 28ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(45, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "actions[np.argmax(res[40])]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KIBtoP2qRrfd",
        "outputId": "4d7316d5-fd8c-443d-adf1-d2b4622f5f90"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Thank_you'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "actions[np.argmax(y_test[40])]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RKbGAtK-R0dE",
        "outputId": "c300f923-b678-4cdd-83a9-180947474563"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Thank_you'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predictions of the model for the test set\n",
        "predicted_values = model.predict(X_test)\n",
        "\n",
        "# Get the predicted classes for all instances in the test set\n",
        "pred_classes = np.argmax(predicted_values, axis=1)\n",
        "\n",
        "# Get the true classes for all instances in the test set\n",
        "true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Compute the accuracy of the model by comparing the predicted and true classes\n",
        "num_correct = np.sum(pred_classes == true_classes)\n",
        "total_instances = len(true_classes)\n",
        "accuracy = num_correct / total_instances\n",
        "\n",
        "print('Accuracy:', accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dIWun8vZf5l",
        "outputId": "b3087315-a170-4730-e6b9-11be4cfaa2c6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 30ms/step\n",
            "Accuracy: 0.9777777777777777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n"
      ],
      "metadata": {
        "id": "WzYof6yVc8VQ"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yhat = model.predict(X_test)\n",
        "\n",
        "ytrue = np.argmax(y_test, axis=1).tolist()\n",
        "\n",
        "yhat = np.argmax(yhat, axis=1).tolist()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4KTkXwddMvh",
        "outputId": "1e17fa3b-88ab-45e4-d137-8012f0ce67a0"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 55ms/step\n"
          ]
        }
      ]
    }
  ]
}