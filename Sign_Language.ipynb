{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPK2vs8QSMc2/Dw6PuFS49k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MowsamR/Sign-Language/blob/main/Sign_Language.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3hplSvKcxMT",
        "outputId": "e7c2033f-da6b-4900-f00d-5589881e487c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.9/dist-packages (4.7.0.72)\n",
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.9.2.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.6/33.6 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.9/dist-packages (from opencv-python) (1.22.4)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.9/dist-packages (from mediapipe) (22.2.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.9/dist-packages (from mediapipe) (3.20.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from mediapipe) (23.3.3)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.9/dist-packages (from mediapipe) (4.7.0.72)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (8.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (5.12.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (1.0.7)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (4.39.3)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (23.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Installing collected packages: mediapipe\n",
            "Successfully installed mediapipe-0.9.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python mediapipe matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "import mediapipe as mp\n",
        "import tensorflow as tf\n"
      ],
      "metadata": {
        "id": "OVV5_LfCd06r"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mp_holistic = mp.solutions.holistic # Holistic model\n",
        "mp_drawing = mp.solutions.drawing_utils # Drawing utilitie"
      ],
      "metadata": {
        "id": "z3RtEi_ld260"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mediapipe_detection(image, model):\n",
        "\timage = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
        "\timage.flags.writeable = False\n",
        "\tresults = model.process(image)\n",
        "\timage.flags.writeable = True\n",
        "\timage = cv.cvtColor(image, cv.COLOR_RGB2BGR)\n",
        "\treturn image, results"
      ],
      "metadata": {
        "id": "JDb5wQyed4OQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_landmarks(image, results):\n",
        "\tmp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1))\n",
        "\tmp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2))\n",
        "\tmp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2))\n",
        "\tmp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2))"
      ],
      "metadata": {
        "id": "kKuQAOcTd5IH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_keypoints(results):\n",
        "\tpose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
        "\tface = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
        "\tleft_hand = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
        "\tleft_hand = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
        "\treturn np.concatenate([pose, face, left_hand, left_hand])\n"
      ],
      "metadata": {
        "id": "1-mM0p4Zd7P7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
        "\t\tsamples_path = os.path.join('Sign', 'Clips')\n",
        "\t\tshould_break = False\n",
        "\t\tfor sample in os.listdir(samples_path):\n",
        "\t\t\tif should_break:\n",
        "\t\t\t\tbreak\n",
        "\n",
        "\t\t\tprint(sample)\n",
        "\t\t\tclips_path = os.path.join(samples_path, sample) \n",
        "      #clips_path = os.path.join(samples_path, sample): This line creates\n",
        "      #a string containing the path to the current file or directory by joining the 'Clips' directory\n",
        "      #path with the name of the current file or directory.\n",
        "\t\t\tif not os.path.isdir(clips_path):\n",
        "\t\t\t\tcontinue\n",
        "\n",
        "\t\t\tfor clip_number, clip in enumerate(os.listdir(clips_path)):\n",
        "\t\t\t\tif should_break:\n",
        "\t\t\t\t\tbreak\n",
        "\n",
        "\t\t\t\tfile = os.path.join(clips_path, clip)\n",
        "\t\t\t\tif not (os.path.isfile(file) and file.endswith('.mp4')):\n",
        "\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\tcap = cv.VideoCapture(file)\n",
        "\n",
        "\t\t\t\tif not cap.isOpened(): \n",
        "\t\t\t\t\tprint(\"Cannot open Video feed\")\n",
        "\t\t\t\t\texit()\n",
        "\t\t\t\tframe_number = 0\n",
        "\t\t\t\twhile True:\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\t# Capture frame-by-frame\n",
        "\t\t\t\t\tret, frame = cap.read()\n",
        "\t\t\t\t\t    \n",
        "\t\t\t\t\t# if frame is read correctly ret is True\n",
        "\t\t\t\t\tif not ret:\n",
        "\t\t\t\t\t\tprint(\"Can't receive frame (stream end?). Exiting ...\")\n",
        "\t\t\t\t\t\tbreak\n",
        "\n",
        "\t\t\t\t\t# Make detections\n",
        "\t\t\t\t\timage, results = mediapipe_detection(frame, holistic)\n",
        "\n",
        "\t\t\t\t\tkey_points = extract_keypoints(results)\n",
        "\t\t\t\t\tprint(key_points)\n",
        "\t\t\t\t\tnpy_path = os.path.join('Dataset', str(sample), str(clip_number), str(frame_number))\n",
        "\t\t\t\t\tnp.save(npy_path, key_points)\n",
        "\n",
        "\t\t\t\t\tdraw_landmarks(image, results)\n",
        "\n",
        "\t\t\t\t\t# Display the resulting frame \n",
        "\t\t\t\t\timage = cv.flip(image, 1)\n",
        "\t\t\t\t\tcv2_imshow(image)\n",
        "\n",
        "\n",
        "\t\t\t\t\tif cv.waitKey(1) == ord('q'):\n",
        "\t\t\t\t\t\tshould_break = True\n",
        "\t\t\t\t\t\tbreak\n",
        "\n",
        "\t\t\t\t\tframe_number += 1\n",
        "\n",
        "\t\t\t\t# When everything done, release the capture\n",
        "\t\t\t\tcap.release() \n",
        "\t\t\t\tcv.destroyAllWindows()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "c5ebwRP2d9lg",
        "outputId": "6a7b11cb-1dd0-4723-f052-3ba5b7d6dd8d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-b691f1b7e3ca>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                 \u001b[0msamples_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Sign'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Clips'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0mshould_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mshould_break\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Sign/Clips'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "!git clone https://github.com/MowsamR/Sign-Language.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qy16zjneeAYr",
        "outputId": "184de79a-a946-4a20-f8be-09ae30a8ae55"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Sign-Language'...\n",
            "remote: Enumerating objects: 4668, done.\u001b[K\n",
            "remote: Counting objects: 100% (4668/4668), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4640/4640), done.\u001b[K\n",
            "remote: Total 4668 (delta 27), reused 4668 (delta 27), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (4668/4668), 29.29 MiB | 24.31 MiB/s, done.\n",
            "Resolving deltas: 100% (27/27), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path for exported data, numpy arrays\n",
        "DATA_PATH = os.path.join('/content', 'Sign-Language', 'Extracted_Keypoints') \n",
        "\n",
        "# Actions that we try to detect\n",
        "actions = np.array(['Good', 'Hello', 'Thank_you'])\n",
        "\n",
        "# This is the number of videos per action \n",
        "no_sequences = 50\n",
        "\n",
        "# Each video has 30 frames\n",
        "sequence_length = 30\n",
        "\n",
        "label_map = {label:num for num, label in enumerate(actions)}\n",
        "\n"
      ],
      "metadata": {
        "id": "HqLDZ1vugJTu"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sequences contains \n",
        "sequences, labels = [], []\n",
        "#Loop through each action\n",
        "for action in actions:\n",
        "  #Loop through each video (no_sequence = video)\n",
        "    for sequence in range(1,no_sequences+1):\n",
        "        window = []\n",
        "        #Loop through each frame in each video (sequence_length = frame)\n",
        "        for frame_num in range(sequence_length):\n",
        "            #Load the data in \"{}.npy\".format(frame_num) into res variable.\n",
        "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
        "            #all frames collected will be stored in window list sequentially until (0-29)\n",
        "            window.append(res)\n",
        "\n",
        "        #once the window list is full (contains 0-29 .npy files), the loop will start again from no_sequence videos.\n",
        "        #all the 0-29 .npy files stored in windows will then be stored in sequences lists in that order until (1-50)\n",
        "        sequences.append(window)\n",
        "\n",
        "        #label_map[action] gets the index for the current action.\n",
        "        #this index will be appended to labels list\n",
        "        labels.append(label_map[action])"
      ],
      "metadata": {
        "id": "j66rX4mfkXc1"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(sequences).shape\n",
        "#150 videos (50 videos each x 3 actions = 150)\n",
        "#30 frames per video\n",
        "#1662 landmarks per frame"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qcabmcQ1von",
        "outputId": "fb7feaa7-758c-4bab-b46b-c52e83cb846f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(150, 30, 1662)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(labels).shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmG4KEc_3L5q",
        "outputId": "5d271a99-5212-40e6-88b2-4489d271a5fd"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(150,)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    }
  ]
}